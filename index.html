<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Secure AI Assistant</title>

  <link rel="manifest" href="./manifest.json" />

  <style>
    body {
      font-family: Arial, sans-serif;
      background: #111;
      color: #eee;
      margin: 0;
      padding: 0;
    }

    header {
      display: flex;
      align-items: center;
      gap: 10px;
      padding: 10px;
      background: #222;
    }

    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: green;
    }

    #status {
      font-weight: bold;
    }

    #initialize {
      margin-left: auto;
      padding: 6px 12px;
      cursor: pointer;
    }

    #vision-container {
      padding: 10px;
    }

    video {
      width: 100%;
      max-width: 400px;
      background: #000;
    }
    <div style="padding:10px;">
  <button id="speechBtn">Start Speech</button>
  <div id="speechText"
       style="margin-top:10px;padding:10px;min-height:60px;">
    (speech text will appear here)
  </div>
    </div>
  </style>
</head>

<body>
  <header>
    <div id="dot" class="status-dot"></div>
    <span id="status">ONLINE</span>
    <button id="initialize">Initialize</button>
    <button id="micBtn">Start Mic</button>
<div id="micLevel" style="width:120px;height:10px;background:#444;border-radius:6px;overflow:hidden;">
  <div id="micBar" style="height:100%;width:0%;background:#00ffcc;"></div>
</div>
  </header>
  
  <div id="vision-container">
    <video id="webcam" autoplay playsinline></video>
  </div>
<div id="speech-ui" style="padding:10px;">
 <button id="speechBtn">Start Speech</button>
  <div id="speechText"
       style="margin-top:10px;
              padding:10px;
              min-height:60px;
              background:#222;
              border-radius:6px;">
    (speech text will appear here)
  </div>
</div>

  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const initBtn = document.getElementById('initialize');
      const statusEl = document.getElementById('status');
     window.setStatus = function (msg) {
  const el = document.getElementById('status');
  if (el) el.textContent = msg;
  console.log('[status]', msg);
};

 let stream = null;

initBtn.addEventListener('click', async () => {
  try {
    const video = document.getElementById('webcam');

    // If already running, stop it (toggle behavior)
    if (stream) {
      stream.getTracks().forEach(t => t.stop());
      stream = null;
      video.srcObject = null;
      setStatus('Camera stopped');
      return;
    }

    setStatus('Requesting camera permission...');

    stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: 'user' }, // 'user' = front camera, 'environment' = back camera
      audio: false
    });

    video.srcObject = stream;
    await video.play();

    setStatus('Camera ON ‚úÖ');
    console.log('Webcam started');
  } catch (e) {
    console.error(e);
    setStatus('Camera failed ‚ùå');
  }
});

    let micStream = null;
let audioCtx = null;
let analyser = null;
let micAnim = null;

const micBtn = document.getElementById('micBtn');
const micBar = document.getElementById('micBar');

function stopMic() {
  if (micAnim) cancelAnimationFrame(micAnim);
  micAnim = null;

  if (micStream) {
    micStream.getTracks().forEach(t => t.stop());
    micStream = null;
  }

  if (audioCtx) {
    audioCtx.close();
    audioCtx = null;
  }

  if (micBar) micBar.style.width = '0%';
  micBtn.textContent = 'Start Mic';
  window.setStatus('Mic stopped');
}

    function startMic() {
  return navigator.mediaDevices.getUserMedia({ audio: true, video: false })
    .then(stream => {
      micStream = stream;
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioCtx.createMediaStreamSource(stream);

      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 1024;

      source.connect(analyser);

      const data = new Uint8Array(analyser.fftSize);

      const tick = () => {
        analyser.getByteTimeDomainData(data);

        // Compute RMS volume (0..1)
        let sum = 0;
        for (let i = 0; i < data.length; i++) {
          const v = (data[i] - 128) / 128;
          sum += v * v;
        }
        const rms = Math.sqrt(sum / data.length);

        // Map RMS to 0..100%
        const pct = Math.min(100, Math.max(0, rms * 220));
        micBar.style.width = pct.toFixed(0) + '%';

        micAnim = requestAnimationFrame(tick);
      };

      tick();

      micBtn.textContent = 'Stop Mic';
      window.setStatus('Mic ON ‚úÖ');
    });
}

micBtn.addEventListener('click', async () => {
  try {
    if (micStream) {
      stopMic();
      return;
    }
    window.setStatus('Requesting mic permission...');
    await startMic();
  } catch (e) {
    console.error(e);
    window.setStatus('Mic failed ‚ùå');
  }
});
// ===== Speech to Text =====
const speechBtn = document.getElementById('speechBtn');
const speechText = document.getElementById('speechText');

if (!speechBtn || !speechText) {
  console.warn('Speech UI missing: add #speechBtn and #speechText in HTML');
} else {
  const SpeechRecognition =
    window.SpeechRecognition || window.webkitSpeechRecognition;

  let recognition = null;
  let recognizing = false;

  if (!SpeechRecognition) {
    speechBtn.disabled = true;
    speechBtn.textContent = 'Speech not supported';
    console.warn('Speech Recognition not supported in this browser');
  } else {
    recognition = new SpeechRecognition();
    recognition.lang = 'en-US';
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onstart = () => {
      recognizing = true;
      speechBtn.textContent = 'Stop Speech';
      if (window.setStatus) window.setStatus('Listening üéôÔ∏è');
    };

    recognition.onend = () => {
      recognizing = false;
      speechBtn.textContent = 'Start Speech';
      if (window.setStatus) window.setStatus('Speech stopped');
    };

    recognition.onerror = (e) => {
      console.error(e);
      if (window.setStatus) window.setStatus('Speech error ‚ùå');
    };

    recognition.onresult = (event) => {
      let text = '';
      for (let i = event.resultIndex; i < event.results.length; i++) {
        text += event.results[i][0].transcript;
      }
      speechText.textContent = text;
    };

    speechBtn.addEventListener('click', () => {
      if (!recognition) {
        console.warn('SpeechRecognition not available');
        if (window.setStatus) window.setStatus('Speech not supported ‚ùå');
        return;
      }
      if (recognizing) recognition.stop();
      else recognition.start();
    });
    
  }
 }
 });     
 </script>
  <script>
async function callLLM() {
  const res = await fetch(
    'https://printable-effective-reaching-davidson.trycloudflare.com/api/generate',
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'deepseek-r1:8b',
        prompt: 'Say hello',
        stream: false
      })
    }
  );

  const data = await res.json();
  console.log(data.response);
}

callLLM();
</script>

</body>
</html>

















